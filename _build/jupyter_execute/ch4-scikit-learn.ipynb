{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32c4d5e9",
   "metadata": {},
   "source": [
    "# Scikit-Learn ve Makine Öğrenmesi\n",
    "\n",
    "Bu bölümde genellikle makine öğrenmesi kütüphanesi olarak bilinen Scikit-Learn kütüphanesinden bahsedilecektir. Makine öğrenmesi uygulamaları için Pandas, NumPy ve Matplotlib kütüphaneleri birlikte kullanılarak bir model geliştirilebilir. Ancak bu durumda makine öğrenmesi uygulaması için fonksiyonları tekrar tekrar yazmak gereklidir. Scikit-Learn kütüphanesi, makine öğrenmesi süreçlerini içerdiği fonksiyonlar ve matematiksel hesaplamaları sayesinde pratik birtakım yöntemler sunmaktadır. Bu kütüphane veri bilimi süreçlerini kolay bir şekilde yerine getirebilmek için tasarlanmıştır. Bu bölümde öncelikle makine öğrenmesi hakkında teorik bilgi verilecektir. Ardından Scikit-Learn kütüphanesi ile makine öğrenmesi konularına bakılacaktır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f005e388",
   "metadata": {},
   "source": [
    "## Makine Öğrenmesine Giriş\n",
    "\n",
    "Makine öğrenmesi ya da yapay öğrenme geçmiş verilerden faydalanarak gelecekteki verilerin öngörülmesi yani geçmiş verilerden öğrenmesi anlamına gelmektedir. Günümüzde verilerin toplanması ve depolanmasından ziyade veriden bilgi üretme çalışmaları daha değerli bir konuma sahiptir. Yani verinin dönüşümü, yapısal olmayan verilerin yapısal formata getirilmesi, verinin kullanılabilir hale getirilmesi, gizli bilgilerin ortaya çıkarılması çalışmaları ön plana çıkmaktadır. Makine öğrenmesi yapay zekanın uygulama alanlarından birisidir. Makine öğrenmesinin daha spesifik bir alanı ise derin öğrenmedir. Derin öğrenmede görüntü işleme, ses tanıma, akıllı robotlar gibi alanlarda çalışmalar gerçekleştirilir. Makine öğrenmesi uygulamalarında ise genellikle veri setlerinden bilgi çıkarılmaya çalışılır. \n",
    "\n",
    "Makine öğrenmesi uygulaması için e-postaların spam veya spam olmama olarak iki ayrılması örnek olarak verilebilir. Daha önceden gönderilen ve kullanıcılar tarafından spam olarak işaretlenen veya işaretlenmeyen e-postalardan çıkarım yapılarak yeni e-postalar için bir sınıflandırma yapılabilir. Bu tip çalışmalar sınıflandırma veya gözetimli öğrenme çalışmaları olarak bilinmektedir. Sınıflandırma çalışmalarında geçmişteki verilerin etiketleri ya da hedef değişkeninin değerlerinin bilindiği varsayılır. Hedef değişkenin değerleri bilinmiyor ve tahmin edilmeye çalışılıyorsa bu tip çalışmalar da gözetimsiz öğrenme çalışmaları olarak bilinmektedir. \n",
    "\n",
    "## Makine Öğrenmesinde Temel Kavramlar\n",
    "\n",
    "Daha önce veri madenciliğinin kalbinde veri vardır denilmişti. Makine öğrenmesinin en önemli kavramı ise öğrenme tanımıdır. Yapay öğrenmede insanlar ya da hayvanların öğrenme mekanizmaları taklit edilerek makineler üzerinde uygulanmak istenir. Makine öğrenmesindeki esas amaç genelleştirme kabiliyeti olan modeller ortaya koymaktır. Örneğin 100 hastaya ait hastalık verilerinin hastaneye yeni gelecek kişilerde de aynı özelliklerin olması durumunda hasta olup olmadığına bakılabilir. Bu işleme genelleştirme işlemi denilir. Hasta veri seti örneğinde olduğu gibi eğer veri seti çıktı değişkenine sahipse bu öğrenme tipine gözetimli öğrenme denilir. Gözetimli öğrenmede sınıflandırma çalışmaları gerçekleştirilir. Veri setindeki gözlemlerin her biri girdi değişkenlerine(x) ve çıktı değişkenine(y) sahiptir. Dolayısıyla, sınıflandırma çalışmalarında x noktalarıyla y noktası açıklanmaya çalışılır. \n",
    "\n",
    "Makine öğrenmesi modellerinde ayrıca çıktı değişkenin sayılabilir ya da sayılamaz olması ile de ilgilenilir. Eğer sayılabilir(kantitatif) çıktı değişkeni varsa bu durumda regresyon çalışmaları gerçekleştirilir. Eğer çıktı değişkeni sayılamaz(kalitatif) veri tipinde ise bu durumda sınıflandırma çalışmaları gerçekleştirilir. Regresyon çalışmalarında girdi değişkeni kategorik veya nümerik veri tipinde olabilir. Ancak çıktı değişkeninin nümerik veri tipinde olması gereklidir. Çıktı değişkeninin nümerik olması halinde eğer sınıflandırma çalışması yapılmak istenirse veriler kategorik hale dönüştürülerek işlem yapılabilir. Örneğin çıktı değişkeni 1-60 arasındaki yaşlardan oluşuyorsa 0-20 çocuk, 20-40 genç, 40 ve sonrası orta yaşlı denilerek bir kategorizasyon yapılabilir. Böylece problem sınıflandırma problemi olarak ele alınabilir. Bu bölümde makine öğrenmesi yaklaşımları gösterilecek ve regresyon analizi ile ilgili detaylı inceleme bir sonraki bölümde verilecektir. \n",
    "\n",
    "Makine öğrenmesi, bir bilgisayar sisteminin veri örnekleri kullanarak kendisini eğitmesi ve belirli bir görevi yerine getirmesi için programlanmasıdır. Bu süreçte kullanılan öğrenme çeşitleri şunlardır:\n",
    "\n",
    "### Gözetimli öğrenme\n",
    "\n",
    "Gözetimli Öğrenme (Supervised Learning): Bu öğrenme türünde, önceden belirlenmiş etiketler (labels) ile veriler kullanılarak, bir model oluşturulur. Bu model, daha sonra yeni verileri sınıflandırmak, tahmin etmek veya keşfetmek için kullanılır. Örnek olarak, bir e-postanın spam veya spam olmadığı gibi bir sınıflandırma problemi gözetimli öğrenme örneklerinden biridir.\n",
    "\n",
    "Sınıflandırma çalışmaları için kullanılacak veri setini oluşturmak veya elde etmek kolay olmaz. Çünkü bazı durumlarda çıktı değişkeninin bilinmesi maliyetli olabilir. Örneğin bir hastalığa ait veri setinde çıktı değişkeni kişinin hasta/hasta değil durumları olsun. Bu durumda hastalık teşhisi gerçekleştikten sonra veri seti oluşabilecektir. Yani veri setindeki her kişi için hastalık teşhisinin yapılmış olması gereklidir. Bu da hastalık teşhisi için zaman ve para maliyetini getirir. Bu nedenle veri setinde çıktı değişkeninin olmaması durumunda da kullanılabilecek modellere ihtiyaç vardır. Bu modeller gözetimsiz öğrenme modelleri olarak bilinir. Gözetimsiz öğrenme modellerinde kullanılan veri setlerine çıktı değişkeni olmadığı için etiketsiz veri setleri de denilebilir. Gözetimsiz öğrenmenin en önemli örneği kümeleme çalışmalarıdır. Kümeleme çalışmalarında, daha önce veriler arasındaki uzaklık ve yakınlık ölçüleri ile veri noktaları arasındaki yakınlıklar ortaya koyularak benzer veriler aynı gruplara atanır. Gözetimsiz öğrenme çalışmaları bir keşif çalışmasıdır. Bu keşif çalışmasında veriler arasındaki ilişkiler, yakınlıklar veya uzaklıklar önemli yer tutar.\n",
    "\n",
    "### Gözetimsiz öğrenme\n",
    "\n",
    "Diğer bir öğrenme çeşidi ise yarı gözetimsiz öğrenmedir. Gözetimsiz Öğrenmede (Unsupervised Learning), verilerdeki örüntüleri ve yapıları ortaya çıkarmak için kullanılır. Bu yöntemde verilerin etiketleri veya sınıflandırmaları yoktur. Örnek olarak, bir müşteri veri setinde benzerlik grupları oluşturmak denetimsiz öğrenme örneklerinden biridir. \n",
    "\n",
    "Gözetimsiz öğrenme algoritmaları, veri kümesindeki örnekler arasındaki benzerlikleri ve farklılıkları belirleyerek veri kümesini önceden belirlenmiş gruplara ayırabilir. Buna kümeleme (clustering) denir. Örneğin, bir müşteri veri kümesi üzerinde yapılan bir kümeleme işlemi, benzer satın alma davranışları sergileyen müşterileri aynı gruba yerleştirebilir.\n",
    "\n",
    "Gözetimsiz öğrenme ayrıca boyut indirgeme işlemlerinde de kullanılır. Bu işlem, veri kümesindeki öznitelik sayısını azaltarak verilerin işlenmesini kolaylaştırır ve daha az gürültülü sonuçlar elde edilir. Örneğin, bir resim veri kümesindeki yüksek boyutlu öznitelikleri, temel özelliklerine indirgenerek daha az boyutlu bir temsil ile ifade edilebilir.\n",
    "\n",
    "Gözetimsiz öğrenme, özellikle büyük veri kümelerinde yapısal olmayan desenleri tespit etmek için kullanılır. Ancak, gözetimsiz öğrenme yöntemleri, gözetimli öğrenme yöntemleri kadar kesin sonuçlar veremeyebilir ve elde edilen sonuçların yorumlanması zor olabilir.\n",
    "\n",
    "### Yarı Gözetimli Öğrenme (Semi-Supervised Learning)\n",
    "\n",
    "Bu öğrenme türünde, verilerin bir kısmı etiketlenmiş, diğer kısmı ise etiketlenmemiştir. Gözetimli ve denetimsiz öğrenme yaklaşımlarını birleştirir ve bir öğrenme modeli oluşturur. Bu model, etiketlenmemiş verileri sınıflandırmak, tahmin etmek veya keşfetmek için kullanılabilir.\n",
    "\n",
    "Etiketli veriler, veri kümesindeki örneklerin doğru çıktı bilgilerinin olduğu verilerdir. Bu verilerin kullanılması, gözetimli öğrenme algoritmalarının eğitilmesine olanak tanır. Ancak, etiketli verilerin toplanması genellikle zaman alıcı ve maliyetlidir. Etiketlenmemiş veriler, doğru çıktı bilgilerinin olmadığı verilerdir. Yani, veri kümesindeki örneklerin etiketleri bilinmemektedir. Yarı gözetimli öğrenme, bu verilerin de kullanılmasını sağlar ve veri kümesinin daha iyi işlenmesine olanak tanır.\n",
    "\n",
    "Yarı gözetimli öğrenme algoritmaları, hem etiketli hem de etiketlenmemiş verileri birleştirerek bir model inşa eder. Etiketlenmemiş verilerin kullanımı, modelin daha iyi özellikler (features) öğrenmesini sağlar ve böylece daha iyi bir performans elde edilir. Bu öğrenme yöntemi, özellikle veri kümesindeki etiketli verilerin sayısının az olduğu durumlarda etkili olabilir.\n",
    "\n",
    "Yarı gözetimli öğrenme, çeşitli uygulamalarda kullanılır. Örneğin, bir web sitesindeki kullanıcıların davranışlarını takip etmek ve kullanıcıların hangi sayfaları ziyaret ettiğini, hangi ürünleri incelediğini vb. öğrenmek için kullanılabilir. Bu sayede, kullanıcılara daha iyi hizmet sunulabilir ve öneriler yapılabilir. Ayrıca, tıbbi görüntülerde yapısal olmayan desenleri tanımlamak ve kanser tanısında yardımcı olmak gibi diğer uygulamalarda da kullanılabilir.\n",
    "\n",
    "### Pekiştirmeli (Takviyeli) öğrenme\n",
    "\n",
    "Pekiştirmeli Öğrenme (Reinforcement Learning) türünde, bir karar verme modeli, verilen bir görev için ödüllendirilir veya cezalandırılır. Bu ödüller veya cezalar, modelin hangi kararları alması gerektiğini öğrenmesine yardımcı olur. Örnek olarak, bir oyun oynayan bir yapay zeka örneği düşünebilirsiniz. Burada, doğru hareketler yaparak oyunu kazanmak için ödüllendirilir, yanlış hareketler ise cezalandırılır. \n",
    "\n",
    "Bu öğrenme türlerinin her biri farklı bir amaca hizmet etmektedir. Gözetimli öğrenme, sınıflandırma veya regresyon problemlerinde kullanılırken, denetimsiz öğrenme, veri analizi veya kümeleme gibi problemlerde kullanılır. Pekiştirmeli öğrenme, genellikle oyunlar, robotik veya otomatik sürüş gibi algoritmik karar problemleri için kullanılır.\n",
    "\n",
    "Bu öğrenme yönteminde, model bir ortamda bir dizi eylem gerçekleştirir ve bu eylemler sonucunda bir ödül alır veya ceza alır. Modelin amacı, uzun vadede en yüksek ödülü alacak eylemleri öğrenmektir. Bu süreç, modelin başarılı eylemleri öğrenmesini ve hatalı eylemlerden kaçınmasını sağlar. Örneğin, bir robotun hareketlerini kontrol etmek için pekiştirmeli öğrenme kullanılabilir. Robot, bir ortamda çeşitli eylemler gerçekleştirir (örneğin, ilerler veya döner) ve bu eylemler sonucunda ödül veya ceza alır (örneğin, hedefe yaklaşır veya duvara çarpar). Bu süreç, robotun zamanla en etkili hareketleri öğrenmesini sağlar. Pekiştirmeli öğrenme, öğrencinin herhangi bir önceden tanımlanmış sonuç etiketine bağlı kalmaksızın, doğrudan çevreye yanıt vermesine izin verir. Bu nedenle, pekiştirmeli öğrenme, özellikle robotik, oyunlar, otomatik araçlar ve hatta tıbbi uygulamalar gibi uygulamalar için ideal bir öğrenme yöntemidir.\n",
    "\n",
    "### Eğitim ve Test Setleri\n",
    "\n",
    "Gözetimli öğrenmede kullanılan veri seti öğrenmenin gerçekleştirildiği eğitim seti ve öğrenmenin test edildiği test seti olarak ikiye ayrılmaktadır. Birinci set eğitim seti, burada algoritmanın öğrenme gerçekleştireceği veriler bulunur. Sınıflandırmada özellikler ve hedef değişkenlerin olduğu bir settir. Test veri seti ise yine aynı şekilde gözlem değerlerinden oluşur. Ancak bu set eğitime girmez. Onun yerine eğitilen algoritmanın ne kadar iyi çalıştığı ile ilgili kullanılır. Eğitim ve test setleri birbirinden bağımsız olmalıdır. Yani ortak veri içermemelidir. Çünkü ortak veri olması durumunda eğitim setinin ezberlenmesi durumu söz konusu olabilir. Makine öğrenmesinde karşılaşılan önemli problemlerden birisi modelin ezberlemesidir. İngilizcede ezberleme olayına \"over-fitting\" denilir. Türkçede aşırı öğrenme olarak da isimlendirilebilir. Model eğitim sürecinde ezberler ise performansı yeni gelecek veriler için daha kötü olacaktır. Ezberlemeden kaçınmak için neler yapılacağı ile ilgili detaylı bilgi daha sonra verilecektir.\n",
    "\n",
    "Bu 2 set dışında ayrıca üçüncü bir setten de bahsedilebilir. O da validasyon veya doğrulama setidir. Bu setin kullanım amacı modelin parametrelerini düzenlemektir. Modelin parametrelerine İngilizcede hyperparameters denilir. Algoritmanın performansı test seti üzerinden değerlendirilir, performans değerlendirmesi doğrulama seti üzerinden yapılmamalıdır. Genel olarak eğitim, test ve doğrulama setlerinin ayrımı için bir oran vermek doğru değildir. Her model için farklılık arz edebilir. Bir oran söylemek şart ise, eğitim seti için %50-60, test seti için %20-30 ve doğrulama seti için %20-30 aralığında veriler kullanılabilir denilebilir.\n",
    "\n",
    "### Çapraz Doğrulama\n",
    "Algoritmanın performansının test edilebilmesi için kullanılabilecek bir yöntemdir. Bu yöntemde veri seti belirli sayıda parçaya bölünür. Bölünen parçalardan bir parça test veri seti olarak kullanılır. Diğer parçalar eğitim veri seti olarak kullanılır. Bu sayede tüm parçalar kullanılacak şekilde eğitimin performansı test edilmiş olunur. Parça sayısına İngilizcede \"fold\" denilir. Yani \"10-folds cross validation\" 10 parçalı çapraz doğrulama demektir. Aşağıdaki görselde çapraz doğrulamaya ilişkin bir şekil paylaşılmıştır. Görüldüğü gibi eşit parçalara bölünen veri setinde tüm parçalar test amacıyla kullanılacak şekilde bir doğrulama gerçekleştirilmektedir.\n",
    "\n",
    "<img src=\"https://i.vgy.me/ySUyQY.png\">\n",
    "\n",
    "### Performans Ölçütleri\n",
    "Eğitim setinde gerçekleşen öğrenmenin test setinde öğrenme derecesi ile performans ölçütleri hesaplanmaktadır. Ele alınan makine öğrenmesi uygulamasına göre performans ölçütleri farklılık göstermektedir. Örneğin kategorik hedef değişkeninin olduğu sınıflandırma problemlerinde genel olarak kullanılan performans ölçütleri için şu örnekler verilebilir.\n",
    "Karışıklık matrisi sınıfların doğru tahmin edilip edilmediği hakkında bilgi veren bir matristir. Aşağıdaki görselde gösterildiği gibi TP(True Pozitive) doğru tahmin edilen pozitif verilerin sayısını, FP(False Positive) yanlış tahmin edilen negatif sayısını, FN(False Negative) yanlış tahmin edilen negatif değerlerin, TN(True Negative) ise doğru tahmin edilen negatif değerlerin sayısını göstermektedir.\n",
    "\n",
    "<img src=\"https://i.vgy.me/VbXn1V.png\">\n",
    "\n",
    "Karışıklık matrisi ile hesaplanacak performans ölçütlerinden ilki doğruluk skorudur. İngilizcede Accuracy Score olarak geçer. Toplam doğru tahminlerin sayısının toplam sayıya bölünerek hesaplanır. En fazla kullanılan performans ölçüsü olmasına rağmen tek başına kullanılması yeterli olmayabilir. Eğer çıktı değerlerinin sayısı birbirinden çok farklı ise doğruluk skoru iyi sonuçlar vermez. Bu durumda kesinlik, duyarlılık ve F1 skoru ölçütlerine bakılabilir. Kesinlik(precision) değerinde pozitif tahmin edilenlerin tüm pozitifler içerisindeki oranı hesaplanır. Kesinlik skoru ile FP yani yanlış tahmin edilen pozitif değerlere dikkat çekilir. Diğer ölçüt ise duyarlılık(recall) performans ölçüsüdür. Bu ölçütte ise yanlış tahmin edilen negatif değerlere dikkat çekilir. İki ölçütün bir arada kullanıldığı F1 skoru ise diğer bir performans ölçüsüdür. Performans ölçütlerinin formülasyonu aşağıda verilmiştir.\n",
    "\n",
    "$$ doğruluk = TP+TN / TP+FP+TN+FN$$\n",
    "$$ kesinlik=TP/(TP+FP)$$\n",
    "$$ duyarlılık=TP/(TP+FN)$$\n",
    "$$ F1=2\\times(kesinlik\\times duyarlılık)/(kesinlik+duyarlılık)$$\n",
    "\n",
    "### Regresyon Analizinde Performans Ölçütleri\n",
    "Regresyon analizinde hedef değişkenler sürekli veriler olduğu için karışıklık matrisi kullanılamaz. Regresyon analizinde gerçekleşen ile tahmin edilen arasındaki farka hata terimi denilir. Hata terimi aşağıdaki görseldeki gibi gösterilebilir.\n",
    "\n",
    "<img src=\"https://i.vgy.me/wx2W3F.png\">\n",
    "\n",
    "Regresyonda kullanılan performans ölçütlerine aşağıda yer verilmiştir. \n",
    "Ortalama Mutlak Hata(Mean Absolute Error): Tahminlerin gerçek değerlerden mutlak farkının ortalamasıdır.\n",
    "Ortalama Mutlak Hata\n",
    "\n",
    "MAE=$$\\frac{1}{N}\\sum_{i=1}^{N}\\left|y_i-\\hat{y}\\right|$$\n",
    "\n",
    "Ortalama Karesel Hata (Mean Squared Error)\n",
    "\n",
    "MSE=$$\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i-\\hat{y}\\right)^2$$\n",
    "\n",
    "Ortalama Karesel Hataların Karekökü ( Root Mean Squared Error)\n",
    "\n",
    "RMSE=$$\\sqrt{\\frac{1}{N}\\sum_{i=1}^{N}\\left(y_i-\\hat{y}\\right)^2}$$\n",
    "\n",
    "R^2 Skoru (Determinasyon Katsayısı)\n",
    "\n",
    "$$1-\\frac{\\sum_{i=1}^{N}\\left(y_i-\\hat{y}\\right)^2}{\\sum_{i=1}^{N}\\left(y_i-\\bar{y}\\right)^2}$$\n",
    "\n",
    "### Makine Öğrenmesi Projeleri için Uygulanabilecek Adımlar\n",
    "Makine öğrenmesindeki temel kavramlardan ve örnek çalışmalardan bahsettikten sonra bir makine öğrenmesi projesi geliştirilirken uygulanabilecek adımlara yer verilebilir. \n",
    "\n",
    "Makine öğrenmesinin ilk aşamasında problemin ortaya koyulması vardır. Bu aşama bilimsel bir çalışma yapılırken genellikle ilk adım olarak bilinir. Bu aşamada problemin çözümündeki amaçlardan, problemin çerçevesinden, sınırlılıklarından ve kapasitesinden bahsedilebilir. Örneğin spam e-postaların sınıflandırılması probleminde amaç gelen kutusuna gelecek e-postaların filtrelenmesi ve spam e-postaların spam klasörüne gitmesini sağlamaktır. Projenin sınırlılıklarına örnek olarak metin içeren e-postalar ya da 50 kelimenin üzerinde metin içeren e-postalar gibi kısıtlamalar eklenebilir. Bu bilgilerin ortaya koyulması ile problemin tanımlanması aşaması geçilmiş olunur. \n",
    "\n",
    "İkinci aşamada verilerin elde edilmesi aşaması vardır. Daha önceki bölümlerde verilerin elde edilebileceği ücretli ve ücretsiz kaynaklardan bahsedilmişti. Kimi durumda veri setinin internet kaynaklarından bulunması mümkün olmayabilir. Proje için özel veri seti üretmek, veri toplamak, verileri istenilen formatta kaydetmek bu aşamada yerine getirilmesi gereken işlemlerden olabilir. \n",
    "Diğer aşamada veri setinin istatistiksel yöntemler ile analiz edilmesi aşaması vardır. Proje için belirlenen veya oluşturulan ilk veri seti analize başlamadan önceki haliyle ham veri seti(raw dataset) olarak isimlendirilir. Ham veri setinin analize hazır hale getirilmesi için daha önce bahsedilen veri önişleme süreçlerinin çalıştırılması gerekmektedir. Ardından veri seti içerisinde anlamlı ilişkilerin ortaya çıkarılması için korelasyon gibi ölçütlerin ortaya koyulması gerekebilir. Eğer bir regresyon çalışması yapılacaksa bağımsız değişkenler arasındaki korelasyonun düşük bağımsız değişken ile bağımlı değişken arasındaki korelasyonun ise yüksek olması istenir. Bu çerçevede bir analiz gerçekleştirilerek bazı bağımsız değişkenlerin analizden çıkarılması sağlanabilir. Ayrıca özellikler setindeki hedef değişkenle alakasız olduğu düşünülen değişkenler de sistemden çıkarılmalıdır. Spam e-posta örneğinde e-postaları gönderen kişilerin cinsiyetine bakmak gereksizdir. Cinsiyet özelliğinin veri setinden ve özellikler kümesinden çıkarılması başta düşünülmesi gereken bir süreç olmalıdır. Bunun gibi varsa başka alakasız özellikler bunların tespit edilerek analizden çıkarılması öğrenmenin kalitesine olumlu yönde etki edecektir. Bu aşamada özellik çıkarımı, özellik mühendisliği gibi yöntemlerden faydalanılmalıdır. Özellikler setine karar verildikten sonra ise veri setinin eğitim ve test seti ya da gerekli görülürse doğrulama veri seti olarak ayrılması işlemine geçilebilir. \n",
    "\n",
    "Sonraki aşamada makine öğrenmesi algoritmasının ortaya koyulması var. Modelin seçilmesi ile modelin diğer modeller ile kıyaslanması, modelin performans değerlendirmesi gibi süreçler bu aşamada yerine getirilir.\n",
    "\n",
    "Modeldeki parametrelerin ayarlanması, optimizasyonu süreçleri, modelin seçilmesinden sonra gerçekleştirilecek aşamadır. Bu aşamada parametre optimizasyonu veya hiperparametrelerin belirlenmesi işlemleri için literatürde geliştirilmiş yöntemlerden faydalanılabilir. Bu yöntemlere örnek olarak “Grid Search” verilebilir. Bu yöntem dışında çeşitli optimizasyon algoritmaları da kullanılabilir. Optimizasyon algoritmaları problemin karmaşıklığına göre sezgisel veya deterministik yöntemler olabilir.\n",
    "\n",
    "Son aşamada ise projenin raporlanması ve sonuçlarının uygun şekilde sunulması işlemleri vardır. Bu aşamada gerekli istatistiksel testler, çapraz doğrulama sonuçları, sonuçların geçerlilik testleri gibi analizlerin verilmesi gerekir. \n",
    "\n",
    "Yukarıda verilen aşamalar ayrıca şekilde özetlenmiştir. Makine öğrenmesi projeleri için bu aşamalar yerine getirilebilir. Tabii bu aşamalar şart olarak söylenemez. Bu aşamalar projeden projeye değişiklik gösterebilir. \n",
    "\n",
    "<img src=\"https://i.vgy.me/66zdqb.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8797e21",
   "metadata": {},
   "source": [
    "### Doğru makine öğrenmesi algoritmasının seçilmesi\n",
    "\n",
    "Makine öğrenmesi algoritmalarını seçerken dikkat edilmesi gereken bazı faktörler vardır:\n",
    "\n",
    "1. Veri türü: Verilerin türü, hangi makine öğrenmesi algoritması seçileceği konusunda önemli bir rol oynar. Sınıflandırma, regresyon veya kümeleme gibi farklı problemler için farklı algoritmalar mevcuttur.\n",
    "\n",
    "2. Veri boyutu: Veri boyutu, hangi algoritmanın seçileceğini belirler. Büyük veri kümeleri için farklı algoritmalar kullanılabilirken, küçük veri kümeleri için farklı algoritmalar daha etkilidir.\n",
    "\n",
    "3. Hız ve performans: Algoritmaların hızı ve performansı, belirli bir veri kümesinde nasıl çalışacağına dair bir fikir verir. Büyük veri kümeleri için daha hızlı algoritmalar tercih edilir.\n",
    "\n",
    "4. Eğitim verileri: Hangi algoritmanın seçileceği, eğitim verilerinin doğru bir şekilde anlaşılmasıyla da ilgilidir. Eğitim verileri, algoritmanın öğrenme sürecini etkiler ve bu nedenle doğru bir şekilde analiz edilmelidir.\n",
    "\n",
    "5. Doğruluk ve sonuçların yorumlanması: Makine öğrenmesi sonuçları doğru bir şekilde yorumlanmalıdır. Farklı algoritmaların doğruluğu farklı olabilir ve sonuçların ne anlama geldiği doğru bir şekilde anlaşılmalıdır.\n",
    "\n",
    "6. Ölçeklenebilirlik: Algoritmanın ölçeklenebilir olması, daha büyük veri kümeleriyle çalışmak isteyenler için önemlidir.\n",
    "\n",
    "7. Parametreler: Makine öğrenmesi algoritmaları, bir dizi parametre ile yapılandırılabilir. Hangi parametrelerin seçileceği, doğru sonuçların elde edilmesi için önemlidir.\n",
    "\n",
    "Bu faktörlerin dikkate alınması, doğru makine öğrenmesi algoritmasının seçilmesine yardımcı olabilir. Ancak, bu seçim, deneyimli bir veri bilimcisi veya makine öğrenmesi uzmanı tarafından yapılmalıdır. Aşağıdaki görselde bir yol haritası gösterilmiştir. Ayrıca bu alanda yazılmış olan [şu makale](https://medium.com/@davidbreton03/a-full-guide-on-choosing-the-right-machine-learning-algorithm-5fa282a0b2a1) de önemli bir izlenim sunacaktır.\n",
    "\n",
    "<img src = \"https://scikit-learn.org/stable/_static/ml_map.png\">\n",
    "\n",
    "### Başarısızlık Nedenleri\n",
    "Makine öğrenmesi çalışmaları birçok denemeyi, deneme ve yanılmayı, tekrarlı çalışmaları, stresli işlemleri kapsayan zor bir iştir. Genel anlamda makine öğrenmesinin zorlukları veri setinden kaynaklı veya seçilen modelden kaynaklı olabilir. Veri setinin doğru seçilmemesi, eksik, hatalı, yanlış veriler içermesi gibi sorunlar makine öğrenmesi çalışmasını da zora sokabilir. Aynı şekilde veri seti doğru şekilde elde edilse de ardından uygulanan modelle ilgili sorunlar ortaya çıkabilir. Makine öğrenmesinde karşılaşılan zorluklar şu şekilde verilebilir.\n",
    "\n",
    "- Yeterli verinin olmaması\n",
    "- Verilerin kaliteli olmaması\n",
    "- Doğru algoritmanın seçilmemesi\n",
    "- Problemin makine öğrenmesi ile çözülmesinin mantıklı olmaması\n",
    "- Aşırı öğrenme(Overfitting)\n",
    "- Az Öğrenme (Underfitting)\n",
    "\n",
    "## Scikit-Learn Kütüphanesi Hakkında\n",
    "\n",
    "Scikit-Learn kütüphanesine geçmeden önce Scikit-Learn ile kullanılan diğer kütüphaneler hakkında aşağıdaki bilgiler verilebilir. Verilen kütüphaneler, bilimsel çalışmalarda sıklıkla kullanıldığı için SciKits (Science Kits – Bilim Araçları) olarak bilinmektedir. SciKits kütüphaneleri aşağıdaki gibi listelenebilir.\n",
    "\n",
    "- NumPy: N-Boyutlu diziler ve hesaplama kolaylığı sağlayan fonksiyonları sayesinde önemli avantajlar sunmaktadır.\n",
    "- Pandas: Seriler ve veri çerçeveleri veri yapıları ile tablo formatındaki verilerin manipülasyonu ve analizi için önemli avantajlar sunmaktadır.\n",
    "- Matplotlib: Verilerin görselleştirilmesi için kullanılmaktadır.\n",
    "- SciPy: Bilimsel hesaplamalar için kullanılan Python kütüphanesidir.\n",
    "- Scikit-Learn kütüphanesi açık kaynak kodlu bir Python kütüphanesidir. İlk olarak 2007 yılında Google Summer of Code etkinliğinde David Cournapeau tarafından geliştirilmiştir. Şu anda Google, the Python Software Foundation ve INRIA tarafından desteklenen 30’a yakın çalışanı ile geliştirilmesi devam etmektedir. Temel olarak NumPy ve SciPy kütüphaneleri üzerine kurulmuştur. Kütüphanenin ana sayfası http://scikit-learn.org/ adresinde bulunmaktadır. \n",
    "\n",
    "Ana sayfada verildiği gibi Scikit-Learn kütüphanesi şu şekilde tanımlanabilir.\n",
    "-\tTahmine dayalı veri analizi için basit ve verimli araçlar\n",
    "-\tHerkes tarafından erişilebilir ve çeşitli bağlamlarda yeniden kullanılabilir.\n",
    "-\tNumPy, SciPy ve Matplotlib üzerine inşa edilmiştir.\n",
    "-\tAçık kaynak kodlu, ticari olarak kullanılabilir - BSD lisansına sahip.\n",
    "Yine kendi sayfasından Scikit-Learn kütüphanesinin çalıştığı alanlar şu şekilde özetlenebilir.\n",
    "\n",
    "| **Uygulama Alanı** | **Tanım**                                                         | **Uygulamalar**                                                                          | **Algoritmalar**                                              |\n",
    "| ------------------ | ----------------------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------- |\n",
    "| Sınıflandırma      | Nesnenin hangi kategoriye ait olduğunu belirleme.                 | İstenmeyen posta algılama, görüntü tanıma.                                               | en yakın komşular, rastgele orman ve daha fazlası...          |\n",
    "| Regresyon          | Bir nesneyle ilişkili sürekli değerli bir özniteliği tahmin etme. | İlaç tepkisi, hisse senedi fiyatları.                                                    | en yakın komşular, rastgele orman ve daha fazlası...          |\n",
    "| Kümeleme           | Benzer nesnelerin kümeler halinde otomatik gruplandırması.        | Müşteri segmentasyonu, Gruplandırma deneme sonuçları                                     | k-Means, spectral clustering, mean-shift,                     |\n",
    "| Boyut Azaltma      | Dikkate alınması gereken rastgele değişkenlerin sayısını azaltma. | Görselleştirme, Artan verimlilik                                                         | k-Means, feature selection, non-negative matrix factorization |\n",
    "| Model Seçme        | Parametreleri ve modelleri karşılaştırma, doğrulama ve seçme.     | Parametre ayarlama ile geliştirilmiş doğruluk                                            | grid search, çapraz doğrulama, metrikler                      |\n",
    "| Veri Önişleme      | Özellik ayıklama ve normalleştirme.                               | Metin gibi giriş verilerini makine öğrenimi algoritmalarıyla kullanmak üzere dönüştürme. | önişleme, özellik çıkarımı                                    |\n",
    "\n",
    "\n",
    "Scikit-Learn kütüphanesinin öne çıkmasının en önemli nedenlerinden birisi de iyi belgelendirilmiş olmasıdır. Makine öğrenmesi metotları kimi zaman karmaşık matematiksel ifadeler içerebilir. Scikit-Learn olabildiğince basit bir şekilde makine öğrenmesi metotlarını sunmaktadır. Böylece ileri düzey bir matematiğe ihtiyaç duymadan da makine öğrenmesi uygulaması geliştirmek mümkün olabilmektedir. \n",
    "\n",
    "Bu özellikleri sayesinde Scikit-Learn birçok önemli işletme tarafından kullanılmaktadır. Örneğin Spotify, Scikit-Learn kütüphanesi ile müşterilerine yeni müzikler önermektedir. Geçmişteki müzik dinlemeleri ve müzik türleri hakkındaki veriler kullanılarak dinleyicilere daha önce dinlemedikleri ve dinlemek isteyebilecekleri müzikler önerilir. Diğer bir örnek de not alma uygulaması olan Evernote uygulamasıdır. Evernote da Scikit-Learn kütüphanesini sıklıkla projelerinde kullanmaktadır. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e51ba06",
   "metadata": {},
   "source": [
    "### Scikit-Learn Kütüphanesinin Yüklenmesi\n",
    "\n",
    "Scikit-Learn kütüphanesini bilgisayara yüklemek için aşağıdaki şekilde gösterilen komutların komut istemcisine veya Anaconda Prompt içerisine yazılması gereklidir. Ancak Anaconda yüklemesi gerçekleştirilince Scikit-Learn kütüphanesi de yüklü gelmektedir. O nedenle yeniden yüklenmesine gerek yoktur. Eğer farklı bir ortamda çalışılacaksa ortam içerisine Scikit-Learn kütüphanesinin eklenmesi için bu yöntem kullanılabilir.\n",
    "\n",
    "<img src=\"https://i.vgy.me/04hZMJ.png\">\n",
    "\n",
    "Yükleme doğrulandıktan sonra Jupyter notebook içerisinde aşağıdaki kodlar yardımıyla Scikit-Learn kütüphanesi çağrılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8064f682",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.24.2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sk.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ed4ded",
   "metadata": {},
   "source": [
    "Kitapta kullanılan versiyon 0.24.1 versiyonudur. İlerleyen kısımlardaki bazı kodlar farklı versiyonlarda doğru çalışmayabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c202bba",
   "metadata": {},
   "source": [
    "### Scikit-Learn Veri Setleri\n",
    "\n",
    "Veri madenciliği yöntemlerinin uygulanabilmesi için veri setlerine ihtiyaç vardır. Veri setleri kaynaklarından birisi de Scikit-Learn veri setleridir. Bu kaynağın içerisinde de bilinen ve sıklıkla kullanılan veri setleri yer almaktadır. Yani Scikit-Learn veri setleri genellikle çok ön işleme aşaması gerektirmeyen, analize hazır veri setleridir. Makine öğrenmesinde kullanılan veri setleri çoğunlukla 2 boyutludur yani tablo formatındadır. Tablodaki her satır bir veriyi temsil eder. Sütunlar ise özellikleri gösterir. Aşağıdaki görselde Boston veri seti için satırlar ve sütunlar gösterilmiştir.\n",
    "\n",
    "Veri setleri Scikit-Learn kütüphanesi ile gelmektedir. Eğer kütüphane yüklemesi tamamlandıysa veri setleri de bilgisayara yüklenmiş olacaktır. Veri setleri bilgisayarda csv dosyaları şeklinde bulunabilir. Bu işlem için izlenmesi gereken yol “ANACONDA YÜKLEME KLASÖRÜ”\\pkgs\\scikit-learn-0.24.1-py37hf11a4ad_0\\Lib\\site-packages\\sklearn\\datasets\\data”.\n",
    "\n",
    "<img src=\"https://i.vgy.me/a0VxlT.png\">\n",
    "Veri setlerine bilgisayardan ulaşma\n",
    "\n",
    "Scikit-Learn veri setleri “toy” veri setleri olarak bilinir. Yani makine öğrenmesine yeni giriş yapacak kişiler için önerilen veri setlerinden bahsedilmektedir. Bu veri setleri hakkında kısaca bilgi verilmesi gerekirse;\n",
    "\n",
    "- Boston Ev Fiyatları (boston_house_prices.csv): Binanın yaşı, dairenin oda sayısı, binanın bulunduğu bölgedeki suç oranı gibi birtakım özelliklere bakılarak Boston eyaletindeki evlerin fiyatlarının bulunduğu veri setidir. Bu özellikler kullanılarak bir evin fiyatı tahmin edilmeye çalışılır.\n",
    "- Meme Kanseri (breast_cancer.csv): Bir tıp veri setidir. \n",
    "- Diyabet (diabetes_data.csv): Diyabet hastalığı veri setidir.\n",
    "- Zambak Çiçeği (iris.csv):  Zambak çiçeğinin türlerinin olduğu veri setidir.\n",
    "- Şarap Verileri (wine_data.csv): Şarap verilerine ait veri setidir.\n",
    "\n",
    "Bu veri setleri Jupyter notebooka çağrılırken sklearn.datasets içerisinden çağrılır. Aşağıdaki yöntem ile tüm veri setleri Jupyter notebook içerisine alınmış olunur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd329ad2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sklearn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6760/662709669.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mboston\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_boston\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboston\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBunch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'sklearn' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "boston = datasets.load_boston()\n",
    "type(boston)\n",
    "sklearn.utils.Bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038ba2e2",
   "metadata": {},
   "source": [
    "Veri setlerinin kendilerine ait “load_VERİSETİ()” şeklinde bir yapısı mevcuttur. Dolayısıyla veri seti çağrılırken “load_boston()” ile çağrıldı. Aynı şekilde eğer meme kanseri veri seti çağrılmak istenseydi “load_breast_cancer()” fonksiyonu ile çağrılması gerekirdi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193f1a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer = datasets.load_breast_cancer()\n",
    "type(breast_cancer)\n",
    "sklearn.utils.Bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e825f2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bunch içerisinde hangi anahtarlar var?\n",
    "boston.keys()\n",
    "dict_keys(['data', 'target', 'feature_names', 'DESCR', 'filename'])\n",
    "\n",
    "# Anahtarların veri yapıları neler?\n",
    "type(boston.data),type(boston.target),type(boston.DESCR),type(boston.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a228eff",
   "metadata": {},
   "source": [
    "Görüldüğü gibi veri setlerinin veri yapısı Bunch olarak gösterilmektedir. Bunch Python sözlüklerinin Scikit-Learn kütüphanesindeki karşılığı olarak görülebilir. Daha önce bahsedildiği gibi sözlük içerisinde anahtar:değer ilişkileri bulunmaktadır. Sözlük içerisindeki anahtar(keys) değerleri girilerek karşılığında gelecek değerler alınır. Bu sözlüğü parçalayarak veri setinin içeriğinden bahsedilmek gerekirse aşağıdaki kodlar kullanılabilir.\n",
    "\n",
    "Boston veri seti için geçerli olan bu anahtarlar diğer veri setleri için de geçerlidir. Anahtarlar genellikle NumPy dizisi veri tipinde tutulmuştur. Anahtarlar hakkında kısaca bilgiler aşağıda verilmiştir.\n",
    "\n",
    "- data (boston.data): Bir NumPy dizisi şeklinde verilerin tutulduğu anahtardır. NumPy dizilerinde sunulan avantajlardan faydalanılarak incelenebilir.\n",
    "- target(boston.target): Hedef değişkeninin tutulduğu anahtardır. Yine NumPy dizisi şeklinde tutulur. Data ve target veri yapıları veri setini oluşturacak şekilde ayarlanmıştır. Aşağıdaki görselde Scikit-Learn kütüphanesinde geçtiği şekilde veri setlerinde kullanılan kavramlar tekrar gösterilmiştir.\n",
    "\n",
    "\n",
    "<img src=\"https://i.vgy.me/SCykJd.png\">\n",
    "\n",
    "### Scikit-Learn Veri Setlerinin Yapısı\n",
    "\n",
    "Boston veri setindeki veriler X değişkenine, hedef değişkeni de y değişkenine kaydedilebilir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b66fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X ve y değişkenleri\n",
    "X = boston.data\n",
    "y = boston.target\n",
    "boston_features = pd.DataFrame(X, columns=boston.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293bd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e175bfe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfy = pd.DataFrame(y,columns=['price'])\n",
    "dfX = pd.DataFrame(X, columns=boston.feature_names)\n",
    "df = dfX.join(dfy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2354f797",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415c264",
   "metadata": {},
   "source": [
    "![](veri_seti_ayir.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bba51fa",
   "metadata": {},
   "source": [
    "## Veri Önişleme Aşaması\n",
    "\n",
    "Tabii, veri önişleme makine öğrenmesinde oldukça önemli bir aşamadır. Veri önişleme, veri setlerini temizleme, düzenleme ve hazırlama sürecidir. Bu süreç, verilerin doğru bir şekilde analiz edilebilmesi ve makine öğrenmesi modellerinin doğru sonuçlar vermesi için gereklidir.\n",
    "\n",
    "Veri önişleme aşamaları şunları içerebilir:\n",
    "\n",
    "1. Veri toplama: Makine öğrenmesi modeli oluşturmak için uygun verilerin toplanması gereklidir. Bu veriler, çeşitli kaynaklardan gelir ve genellikle büyük miktarda veri içerir.\n",
    "\n",
    "2. Veri ön işleme: Verilerin temizlenmesi ve hazırlanması için birkaç adım gereklidir. Bu adımlar, verilerin düzenlenmesini, eksik verilerin giderilmesini ve verilerin normalleştirilmesini içerebilir. Ayrıca, verilerin özelliklerinin seçimi de bu adımda gerçekleştirilir.\n",
    "\n",
    "3. Veri bölme: Veri setleri genellikle eğitim, test ve doğrulama setleri olarak bölünür. Eğitim seti, modelin eğitiminde kullanılır, test seti modelin doğruluğunu kontrol etmek için kullanılır ve doğrulama seti, modelin performansını izlemek için kullanılır.\n",
    "\n",
    "4. Veri özellikleri seçimi: Veri setleri genellikle birçok özelliğe sahiptir. Bu özelliklerin sayısı, modelin karmaşıklığını artırabilir ve aynı zamanda modelin performansını düşürebilir. Bu nedenle, veri özellikleri seçimi yapılırken, gereksiz özelliklerin çıkarılması ve yalnızca önemli özelliklerin kullanılması gereklidir.\n",
    "\n",
    "5. Veri normalleştirme: Veri setleri, farklı aralıklarda olabilen özellikler içerebilir. Bu nedenle, verilerin normalleştirilmesi gereklidir. Normalizasyon işlemi, verilerin ortalama değerinin sıfır ve standart sapmasının bir olduğu bir ölçekte yeniden ölçeklendirilmesini içerir.\n",
    "\n",
    "Veri önişleme, makine öğrenmesinde oldukça önemlidir ve doğru bir şekilde yapılması, modelin doğruluğunu artırabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b3465c",
   "metadata": {},
   "source": [
    "### Eğitim ve test setlerinin ayrılması\n",
    "\n",
    "Makine öğrenmesi modellerinin performansını ölçmek ve doğru sonuçlar elde etmek için, veriler eğitim seti ve test seti olarak ayrılmalıdır. Eğitim seti, modelin öğrenme sürecinde kullanılan veri kümesidir ve test seti, modelin doğruluğunu ölçmek için kullanılan veri kümesidir.\n",
    "\n",
    "Eğitim seti, modelin öğrenmesinde kullanılan verilerdir. Model, bu verileri kullanarak özelliklerin arasındaki ilişkileri öğrenir ve sonunda bir tahmin yapmak için bu özellikleri kullanabilir. Eğitim setinin yeterince büyük ve temsil edici olması, modelin doğru bir şekilde öğrenmesi için önemlidir.\n",
    "\n",
    "Test seti, modelin performansını ölçmek için kullanılan verilerdir. Model eğitildikten sonra, test setindeki verileri kullanarak tahminler yaparız ve gerçek sonuçlarla karşılaştırırız. Bu sayede modelin doğruluğunu ölçeriz ve gerektiğinde iyileştirmeler yaparız.\n",
    "\n",
    "Veri kümesinin eğitim seti ve test seti olarak nasıl ayrılacağına karar vermek için farklı yöntemler kullanılabilir. En yaygın yöntem, verilerin belirli bir oranda rastgele bölünmesidir. Örneğin, verilerin %80'i eğitim seti olarak kullanılırken, %20'si test seti olarak kullanılabilir.\n",
    "\n",
    "Eğitim ve test setleri ayrıldıktan sonra, eğitim seti üzerinde model eğitilir ve ardından test seti üzerinde doğrulama yapılır. Bu işlem, modelin gerçek dünya verilerinde nasıl performans gösterdiğini anlamak için önemlidir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e14a825",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b02109",
   "metadata": {},
   "source": [
    "Bu kodlar, Scikit-learn kütüphanesinin `model_selection` modülünden `train_test_split` fonksiyonunu kullanarak, veri setini rastgele eğitim ve test alt kümelerine ayırmak için kullanılır.\n",
    "\n",
    "`X` özellik matrisini, `y` hedef değişkenini ve `test_size` parametresi olarak verilen oranı kullanarak, veri setini eğitim ve test setleri olarak ayırır. `random_state` parametresi ise her seferinde aynı rastgele bölmenin oluşturulmasını sağlar.\n",
    "\n",
    "Fonksiyon çağrısı sonucunda, `X_train` ve `y_train` eğitim setini, `X_test` ve `y_test` ise test setini temsil eder. Bu şekilde, veri seti iki ayrı alt kümeye ayrılarak, makine öğrenmesi modelinin eğitiminde kullanılan veri ile test edilmesi için kullanılan veri ayrılmış olur."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0f9ed5",
   "metadata": {},
   "source": [
    "### Eksik veriler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a415e91b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db0aa283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# özet bilgileri sunalım.\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67ce3da",
   "metadata": {},
   "source": [
    "### Ölçeklendirme\n",
    "\n",
    "Makine öğrenmesi modellerinin performansı, verilerin ölçeği ile doğrudan ilişkilidir. Ölçeklendirme, verilerin farklı ölçeklerde veya aralıklarda olduğu durumlarda önemlidir. Örneğin, bir veri kümesinde bir özellik binlerce dolarlık aralıklarda iken, diğer bir özellik 0 ile 1 arasında bir aralığa sahip olabilir. Bu durumda, doğru sonuçlar almak için verilerin ölçeklendirilmesi gerekebilir.\n",
    "\n",
    "Ölçeklendirme işlemi, verilerin belirli bir aralığa veya dağılıma getirilmesini içerir. Verileri ölçeklendirmek için yaygın olarak kullanılan iki yöntem vardır:\n",
    "\n",
    "- Normalizasyon: Verilerin belirli bir aralığa sığdırılması için kullanılır. Örneğin, verilerin 0 ile 1 arasında ölçeklendirilmesi için Min-Max normalizasyonu kullanılabilir.\n",
    "\n",
    "- Standartlaştırma: Verilerin bir standart dağılıma sahip olacak şekilde ölçeklendirilmesini sağlar. Bu, verilerin ortalama değerinden çıkarılması ve standart sapmaya bölünmesi ile yapılır."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51af2708",
   "metadata": {},
   "source": [
    "### MinMaxScaler ile Veri Ölçeklendirme\n",
    "\n",
    "- `fit()`: ölçeklemede belirli bir verinin ortalamasını ve std sapmasını hesaplamak için kullanılır. \n",
    "- `transform()`: fit() yöntemi ile hesaplanan ortalama ve std sapma kullanılarak ölçekleme gerçekleştirmek için kullanılır.\n",
    "- The `fit_transform()` hem fit hem de transform eder.\n",
    "\n",
    "$$x_{std} = \\frac{x - x_{\\min (\\text{axis}=0)}}{x_{\\max (\\text{axis}=0)} - x_{\\min (\\text{axis}=0)}}$$\n",
    "\n",
    "$$x_{scaled} = x_{std} * (max – min) + min$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0042ebf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "mm_scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3514d5a5",
   "metadata": {},
   "source": [
    "Ölçeklendirme, yalnızca eğitim kümesindeki verileri kullanarak, verileri eğitim ve test kümesi arasında böldükten sonra yapılmalıdır. Çünkü test seti görülmeyen yani yeni verileri göstermektedir. Eğitime test verilerinin katılmaması gerekir. Eğitimden önce ölçeklendirme yapmak bu nedenle doğru olmayacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509f8a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = mm_scaler.fit_transform(X_train)\n",
    "\n",
    "print(mm_scaler.min_)\n",
    "print(mm_scaler.scale_)\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "print(X_train_scaled[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36120da",
   "metadata": {},
   "source": [
    "### Aykırı Veri Analizi\n",
    "\n",
    "Aykırı veriler, veri kümesinde diğer gözlemlerden önemli ölçüde farklı olan nadir verilerdir. Aykırı veriler, veri analizi sonuçlarını ciddi şekilde etkileyebilir ve yanıltıcı sonuçlara yol açabilir.\n",
    "\n",
    "Aykırı veri analizi, aykırı verilerin tespit edilmesi, analiz edilmesi ve uygun bir şekilde ele alınması sürecidir. Aykırı veri analizi için bazı yaygın yöntemler şunlardır:\n",
    "\n",
    "- Box plot: Verilerin dağılımını görselleştirmek için kullanılır. Aykırı veriler, kutunun altında veya üstünde yer alırlar.\n",
    "\n",
    "- Z-skoru: Verilerin ortalamasından kaç standart sapma uzakta olduklarını hesaplayan bir istatistik yöntemidir. Z-skoru, belirli bir eşik değerinin üzerinde olan verileri aykırı olarak işaretleyebilir.\n",
    "\n",
    "- IQR yöntemi: Interquartile range (IQR) yöntemi, verilerin ortalamadan ayrılmasına dayanır ve verilerin Q1 ve Q3 çeyreklerindeki değerlere göre ölçeklendirilmesini kullanır. Aykırı veriler, Q1 ve Q3'ün birkaç katı dışında olan veriler olarak tanı"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e74e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "fig = plt.figure(figsize=(10, 10))\n",
    "sns.heatmap(df.corr(),center=0, vmin=-1, vmax=1, square=True, annot=True,cbar_kws={'shrink': 0.8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05480e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape # Orijinal veri seti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b141e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape # eğitim veriseti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8896d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape, y_test.shape # test veri seti"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa26a794",
   "metadata": {},
   "source": [
    "### Doğrusal Regresyon\n",
    "\n",
    "Doğrusal regresyon, iki değişken arasındaki ilişkiyi anlamak için kullanılan bir istatistiksel modellemedir. Bu ilişki doğrusal olarak ifade edilir ve bir çıktı değişkeni (bağımlı değişken) ile bir veya daha fazla girdi değişkeni (bağımsız değişkenler) arasındaki ilişkiyi tanımlar. Doğrusal regresyon, özellikle sürekli sayısal verilerde, bir değişkenin diğer değişkenlerle olan ilişkisini anlamak ve tahminler yapmak için sıklıkla kullanılan bir yöntemdir. Doğrusal regresyon modelleri, verilerdeki değişkenliği açıklayan en az kareler yöntemi kullanılarak elde edilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a9e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5a5358",
   "metadata": {},
   "source": [
    "Bu kodlar, doğrusal regresyon modeli oluşturma ve eğitme işlemini gerçekleştirir. İlk olarak, `sklearn.linear_model` kütüphanesinden `LinearRegression` sınıfı çağrılır ve `model` adı verilen bir örnek oluşturulur. Daha sonra, `fit()` fonksiyonu kullanılarak oluşturulan model eğitilir. Bu işlem, `X_train_scaled` ve `y_train` veri setleri üzerinde gerçekleştirilir.\n",
    "\n",
    "Ayrıca, doğrusal regresyon modelinin performansını değerlendirmek için `sklearn.metrics` kütüphanesinden `r2_score`, `mean_absolute_error` ve `mean_squared_error` fonksiyonları çağrılır. Bu fonksiyonlar, sırasıyla, R-kare skoru, ortalama mutlak hata ve ortalama karesel hata değerlerini hesaplar. Bu değerler, modelin doğruluğunu değerlendirmek için kullanılabilir.\n",
    "\n",
    "Özetle, bu kodlar doğrusal regresyon modelini oluşturur, eğitir ve performansını değerlendirir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9584ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"katsayılar: \", model.coef_)\n",
    "print(\"kesme terimi: \", model.intercept_)\n",
    "print(\"katsayı sayısı: \", model.coef_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32353da",
   "metadata": {},
   "source": [
    "Burada:\n",
    "\n",
    "`model.coef_` ifadesi, öğrenilmiş regresyon modelinin her bir öznitelik için katsayılarını içeren bir NumPy dizisini döndürür.\n",
    "\n",
    "`model.intercept_` ifadesi, öğrenilmiş regresyon modelinin kesme terimini döndürür.\n",
    "\n",
    "`model.coef_.shape` ifadesi, katsayı dizisinin şeklini gösterir, yani modelde kaç öznitelik olduğunu gösterir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4276d95b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_scaled = mm_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b37539",
   "metadata": {},
   "source": [
    "`mm_scaler.transform(X_test)` kodu, önceden oluşturulan `MinMaxScaler` nesnesi olan `mm_scaler`'ı kullanarak test verilerinin ölçeklendirilmesini sağlar. `X_test` veri seti, önceden eğitilen ölçekleyici nesnesi `mm_scaler` ile ölçeklendirilir ve ölçeklendirilmiş test verileri `X_test_scaled` olarak kaydedilir. Bu, eğitim verileriyle aynı ölçeklendirme işleminin kullanıldığından emin olmak için önemlidir ve daha sonra test verileri üzerinde tahmin yapmak için kullanılacaktır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a731a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6517706f",
   "metadata": {},
   "source": [
    "Modelin `predict()` metodu kullanılarak, önceden ölçeklendirilmiş test verileri olan `X_test_scaled` argümanı modelin tahmin etmesi için kullanılır ve sonuç `y_predict` değişkeninde saklanır. Bu sonuçlar daha sonra modelin performansını değerlendirmek için gerçek test hedefleri `y_test` ile karşılaştırılabilir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c457c3ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438376e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a364c7",
   "metadata": {},
   "source": [
    "## Modelin Değerlendirilmesi\n",
    "\n",
    "Modelin değerlendirmesi, öncelikle belirlenen performans metrikleri kullanılarak yapılır. Bu metrikler, modelin ne kadar doğru sonuçlar verdiğini ölçer. Örneğin, sınıflandırma problemleri için doğruluk (accuracy), hassasiyet (precision), duyarlılık (recall) ve F1-score kullanılabilir. Regresyon problemleri için ise ortalama mutlak hata (mean absolute error), ortalama karesel hata (mean squared error) ve belki de belirlenen bir eşiğe göre doğruluğu ölçen bir metrik kullanılabilir.\n",
    "\n",
    "Değerlendirme işlemi, eğitim verileri üzerinden eğitilen modelin test verileri üzerinde nasıl performans gösterdiğini ölçerek yapılır. Bunun için, öncelikle test verileri üzerinde modelin tahminleri hesaplanır ve gerçek sonuçlarla karşılaştırılır. Bu karşılaştırma sonucunda elde edilen performans metrikleri, modelin performansını değerlendirmeye yardımcı olur. \n",
    "\n",
    "Ayrıca, bazı durumlarda çapraz doğrulama (cross-validation) gibi teknikler de kullanılabilir. Bu teknikler, farklı veri parçaları üzerinde modelin performansını değerlendirerek, sonuçların daha güvenilir olmasını sağlar. Bu örnekte R^2 kullanılacaktır. `score()` fonksiyonu determinasyon katsayısını verir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b9ef46",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c69cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determinasyon katsayısını verir.\n",
    "model.score(X_train_scaled,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c696565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure the performance of the model\n",
    "mse = mean_squared_error(y_test, y_predict)\n",
    "rmse = np.sqrt(mse)\n",
    "print(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9cd936",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd7a33f",
   "metadata": {},
   "source": [
    "Diğer metrikler için [bu sayfadan](https://scikit-learn.org/stable/modules/classes.html#regression-metrics) daha fazla bilgi alınabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cfeacf",
   "metadata": {},
   "source": [
    "## Tahminlerin karşılaştırılması\n",
    "\n",
    "Makine öğrenmesinde model performansının değerlendirilmesi için farklı metrikler kullanılır. Bu metrikler, modelin tahminlerinin gerçek değerlerle ne kadar iyi eşleştiğini ve ne kadar doğru olduğunu ölçer. \n",
    "\n",
    "Bir sınıflandırma modeli için kullanılan bazı performans metrikleri şunlardır:\n",
    "\n",
    "- Confusion Matrix (Karmaşıklık Matrisi): Modelin doğruluğunu değerlendirmek için gerçek ve tahmini sınıfları görsel olarak gösterir.\n",
    "- Accuracy (Doğruluk): Tahmin edilen sınıfların doğru sınıfların yüzdesi olarak hesaplanır.\n",
    "- Precision (Hassasiyet): Pozitif olarak tahmin edilen örneklerin gerçekten pozitif olanların yüzdesi olarak hesaplanır.\n",
    "- Recall (Duyarlılık): Gerçekten pozitif olan örneklerin, pozitif olarak tahmin edilenlerin yüzdesi olarak hesaplanır.\n",
    "- F1 Score: Precision ve recall metriklerinin harmonik ortalamasıdır. Bir modelin hem hassasiyet hem de duyarlılık açısından iyi performans göstermesi gerektiğinde kullanılır.\n",
    "\n",
    "Regresyon modelleri için ise kullanılan bazı performans metrikleri şunlardır:\n",
    "\n",
    "- Mean Squared Error (MSE): Gerçek ve tahmin edilen değerler arasındaki farkların karelerinin ortalamasını hesaplar.\n",
    "- Root Mean Squared Error (RMSE): MSE'nin karekökünü alarak elde edilir. Gerçek ve tahmin edilen değerler arasındaki farkın ortalamasıdır.\n",
    "- R-Squared (R²): Tahmin edilen değerlerin gerçek değerlerin varyansını açıklama yüzdesini hesaplar. 1'e yakın bir R² değeri, modelin iyi bir uyum sağladığını gösterir.\n",
    "\n",
    "Bu metrikler, modelin başarısını objektif bir şekilde değerlendirmemizi sağlar ve geliştirme sürecinde ne yapmamız gerektiği hakkında ipuçları sağlar. Diğer yöntemde ise görsel karşılaştırma, tahminlerin gerçek değerlerle karşılaştırılması için kullanılan bir yöntemdir. Bu yöntemde genellikle çizgi grafikleri veya dağılım grafikleri kullanılır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b6b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04684f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(y_test, label='gerçek değerler')\n",
    "plt.plot(y_predict, label='tahmin değerleri');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca02a04",
   "metadata": {},
   "source": [
    "Bu kodlar matplotlib kütüphanesi ile birlikte kullanılarak gerçek ve tahmin edilen değerlerin görselleştirilmesini sağlar. \n",
    "\n",
    "`plt.plot(y_test, label='gerçek değerler')` kodu gerçek değerlerin grafiğini çizerken `plt.plot(y_predict, label='tahmin değerleri')` kodu ise tahmin edilen değerlerin grafiğini çizer. Her iki grafiği karşılaştırmak için aynı grafik üzerinde çizilir ve her bir grafiğin etiketi belirtilir.\n",
    "\n",
    "Bu şekilde, gerçek ve tahmin edilen değerlerin benzerlikleri veya farklılıkları hakkında görsel bir karşılaştırma yapılabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23360dd",
   "metadata": {},
   "source": [
    "## Sonuç\n",
    "\n",
    "Bu bölümde, makine öğrenmesinde veri önişleme ve model değerlendirme konuları ele alınmıştır. Veri önişleme aşamaları, verilerin temizlenmesi, ölçeklendirilmesi, aykırı veri analizi, eğitim ve test setlerinin ayrılması ve daha fazlasını içerir. Model değerlendirme, model performansının değerlendirilmesi, tahminlerin gerçek değerlerle karşılaştırılması, görsel karşılaştırma gibi konuları kapsar. Bu bölümde kullanılan örneklerde, sklearn kütüphanesi kullanılarak veri önişleme işlemleri ve model değerlendirme teknikleri uygulanmıştır. Diğer makine öğrenmesi algoritmaları ile ilgili geliştirilecek uygulamalara ilerleyen bölümlerde yer verilecektir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e91f2c",
   "metadata": {},
   "source": [
    "## Kaynaklar\n",
    "\n",
    "Buitinck, L., Louppe, G., Blondel, M., et al. “API design for machine learning software: experiences from the scikit-learn project”, ECML PKDD Workshop: Languages for Data Mining and Machine Learning, pp. 108–122 (2013)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126d076c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}